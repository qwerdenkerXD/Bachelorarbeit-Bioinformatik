\section{Diskussion} % (fold)
    \label{sec:diskussion}
    Die Entwicklung von \protfin\ hat das Ziel, eine mögliche Alternative zu Alignment-basierten Verwandtschaftsanalysen zwischen Proteinen zu erhalten. Der zugundeliegende Algorithmus ist dabei von SHAZAM inspiriert, welches Musik erkennt, indem es Tonaufnahmen nicht auf direkte Ähnlichkeit vergleicht, wie es bei einem Alignment der Fall wäre, sondern durch Vergleich der periodischen Signale innerhalb der Aufnahmen. Dies wurde nun halbwegs erfolgreich bei \protfin\ umgesetzt. Halbwegs deswegen, weil die Erkennung von Proteinen zwar funktioniert, dafür aber die Performanz hinsichtlich Speicherbedarf der Datenbanken und daraus resultierenden hohen Laufzeiten nicht gut ist. Außerdem fehlt eine robuste und zuverlässige Möglichkeit, verwandte Proteine zu identifizieren. Um diesen Mangeln entgegenzuwirken, wurden \theexperiment\ verschiedene Experimente entwickelt, die zum Ziel haben, nur möglichst signifikante Signale in die Datenbank einfließen zu lassen.

    Verglichen mit den Ergebnissen der vorigen Version von \protfin\ in \autoref{fig:prev_results}, wurde in allen angesetzten Experimenten eine Verbesserung in den Datenbankgrößen erzielt, abgesehen von \acf{FG} 10, welche an Eindeutigkeit der Ergebnisse verloren hat. Es ist hier aber wichtig zu bemerken, dass in der Vorversion das Matching lediglich für nur einen \acf{KF} durchgeführt wurde. Daher ergibt der Verlust bei \ac{FG} 10 Sinn, da die so wenigen infrage kommenden Frequenzen trotzdem auf über 100 \acs{MB} aufgebläht wurden. Bezüglich der Schärfe lässt sich hier leider kein Vergleich vornehmen, da dieses Maß in der Vorversion noch nicht implementiert war. Ebenso stammt der Ansatz des Family-Matchings auch erst aus dieser Arbeit.

    Das Sampling scheint eine gute Idee gewesen zu sein, um die Frequenzselektion signifikanter zu gestalten, ohne dabei willkürlich Information zu entfernen. In \autoref{fig:frequencies_uniref} ist der Effekt auf die Wahl deutlich zu erkennen. Interessant hierbei ist, dass es für mehrere Signifikanzniveaus in jeder 10. Aminosäure ein periodisches Signal gibt. Das betrifft bei einem Fenster der Größe 30 zwar nur drei und ist vielleicht in den \acf{TP} begründet, aber könnte möglicherweise auf ein konzeptuelles Problem hindeuten. Für zukünftige Ergebnisse sollte das im Hinterkopf behalten werden.

    Bezüglich der Werte für $\alpha \le 0.01\%$ ist es zweifelhaft, ob bei diesen niedrigen Häufigkeiten unter 100 wirklich alle \ac{TP} abgedeckt werden können. Für $\alpha=5\%$ sieht das Matching in \autoref{fig:uniref90.sp} jedenfalls noch in Ordnung aus. Da die Datenbankgrößen dennoch recht hoch sind, wäre hier nur eine Reduktion über andere Ansätze denkbar, wie zum Beispiel das Filtern von Hashes in \autoref{exp:filter_hashes}. Die Ergebnisse dort zeigen, dass auf diese Weise effektiv kleinere Datenbanken erzielt werden, wobei die Schärfe und Identifikationsrate scheinbar verlustfrei bleiben. Da bei dem Experiment zudem die gelernten Grenzwerte das alleinige Selektionskriterium darstellen, im Gegensatz zur Kombination mit Wahl lokaler Maxima wie bei \autoref{exp:uniref90}, sollte dies für die Ergebnisse in \autoref{fig:uniref90.sp} zu einer Verbesserung führen. \autoref{fig:filter_hashes.fam} betrachtend, scheint wohl ein zu behaltendes Quantil der Hashes von $10\%$ geeignet zu sein. Allerdings ist der steigende F-Score mit kleiner werdendem Quantil dadurch zu erklären, dass die Wahrscheinlichkeit auf falsche Treffer sinkt, wenn weniger Hashes behalten werden. Die Mitglieder der Familie sind auf jeden Fall enthalten, sofern es Hashes gibt, die sie sich teilen. Die Auswertung der Ergebnisse des Family-Matchings sollte dahingehend erweitert werden, dass angegeben wird, wie viele Familien keine Hashes hatten und was darin die jeweilige Mindestzahl an Hashes ist, die ein Mitglied hat, damit die Darstellung der Ergebnisse weniger irreführend gestaltet werden kann. Ebenso ist es denkbar, dass das Verhältnis von mittlerer Hashanzahl in der Familie und der tatsächlichen Menge geteilter Hashes als Bewertungsmaß neben dem F-Score einfließt.

    In \autoref{exp:target_zone} wird dieselbe Selektionsmethode wie in \autoref{exp:uniref90} verwendet. Es zeigt sich, dass eine \acf{TZ} von 8, also einem Bedarf von 3 Bit, vollkommen ausreichend für die Identifikation ist, welche in ihrer Bewertung keine Einbußen hat und eine Datenbankgröße von medialen etwa 100 \acs{MB} zur Folge hat.

    Bei \autoref{exp:selection_method} hat sich offenbar nur eine Methode als tauglich erwiesen, nämlich die Frequenzen danach zu wählen, wie stark deren Amplituden von den gelernten Grenzwerten in \autoref{exp:uniref90} abweichen. Die Datenbankgrößen sind sehr gut, wenn man bedenkt, dass hier keine Hashes herausgefiltert wurden, sondern das lediglich über diese Selektion mit \ac{TZ} 8 erzielt wurde. Der Parameter $k=3$ scheint ganz geeignet zu sein, die Werte beim Single-Protein-Matching sind da am besten. Beim Family-Matching beeinflusst $k$ scheinbar nicht allzu viel, wenn die Ergebnisse nicht irreführend sind, nur bei \ac{FG} 50 gibt es offenbar einen erhöhten F-Score. Für die weitere Entwicklung sollte diese Selektionsmethode auf jeden Fall in Betracht gezogen werden. Bei Durchführungen, die weiterhin $k$-Werte austesten, reicht es, nur ein $k \in \{0, 1\}$ zu testen, da die Ergebnisse immer identisch sind. Dies liegt daran, dass die Frequenzen nach lokalen Maxima ausgewählt werden, was bedeutet, dass die Randfrequenzen niemals infrage kommen können, da ihr Extremverhalten nur von einer Seite betrachtet werden kann. Wenn also $k=1$ gilt, wird eine Frequenz ignoriert, die sowieso niemals gewählt wird.

    Was in den Ergebnissen fehlt, ist beim Single-Protein-Matching der Bezug zur Familienähnlichkeit. Aktuell wird lediglich betrachtet, ob das Protein selbst identifiziert wurde und wie weit sich der Score von den Treffern abhebt, die nicht in der Familie sind. Letzteres wird durch die Schärfe abgebildet, die den Abstand prozentual angibt, also wie viel höher der Score ist. Dieser Abstand soll möglichst hoch sein, der zu den Familienmitgliedern hingegen nicht. Die Schärfe müsste um diese Information erweitert werden, was sich folgendermaßen formulieren lässt:
    \begin{equation}
        \label{equ:}
        \begin{split}
            scores\_trp &= \{\emph{S1}(t) \cdot JSI(t) \mid t \in TrP\}\\
            scores\_fp &= \{\emph{S1}(f) \cdot JSI(f) \mid f \in FP\}\\
            dist\_nicht\_familie &= \frac{\texttt{max}(scores\_trp) - \texttt{max}(scores\_fp)}{\texttt{max}(scores\_trp)}\\
            dist\_familie &= \frac{\texttt{max}(scores\_trp) - \texttt{mean}(scores\_trp \setminus \{\texttt{max}(scores\_trp)\})}{\texttt{max}(scores\_trp)}\\
            \emph{Schärfe} &= dist\_nicht\_familie \cdot (1 - dist\_familie)
        \end{split}
    \end{equation}

    $TrP$ sind die Treffer innerhalb der Familie und $FP$ die anderen. Der Abstand ($dist$) zu den $TrP$ ist, wie sehr sich der beste Score der Familie prozentual von allen anderen Familienmitgliedern abhebt. Damit dieser Abstand in die Schärfe minimierend einfließt, wird diese wie bisher berechnet und anschließend mit der Umkehrung des Familienabstands multipliziert, also wie nah der beste Score den $TrP$ ist. Auf diese Weise wäre ein hoher Abstand zu $FP$ bei ebenso hohem Abstand zu $TrP$ trotzdem schlecht bewertet. Gleiches gilt umgekehrt, dass eine hohe Nähe zu den Familienmitgliedern auch schlecht bewertet wird, wenn die $FP$ ebenso nah sind. Gegebenenfalls sollte der Abstand zu den $FP$ auch über den Mittelwert berechnet werden anstelle des Maximums, damit die Schärfe robuster gegenüber Ausreißern der $FP$ ist.

    \subsection*{Ausblick} % (fold)
        \label{sub:ausblick}
        Den bisherigen Ergebnissen nach, hat \protfin\ das Potential, eines Tages seinem Zweck gerecht zu werden. Aktuell ist dies zwar noch nicht der Fall, aber die Experimente zeigen, dass es Wege gibt, dem Ziel nahezukommen. Auch möglich ist, dass der aktuelle Ansatz einfach nicht funktioniert, weil die aus den Sequenzen erhaltenen Vektoren zu kurz sind oder ein anderer noch ungesehener Fall vorliegt, der die Verwandtschaftserkennung verhindert.

        Andere Ansätze gibt es auf jeden Fall:
        \begin{enumerate}
            \item Der für mich anfangs intuitivste Weg war, die Sequenzen in echte Musik zu übersetzen, sodass SHAZAM direkt darauf angewandt werden könnte. Im Anhang (siehe \autoref{sec:anhang}) ist eine Beispielmusikdatei, die aus einer Aminosäuresequenz generiert wurde, wobei jede Aminosäure in einem Akkord aus \acp{KF} entsprach. Problem hierbei war ebenso die Datenbankgröße, da Musik komplexer als Text ist. Dennoch ist der Kern der Idee vielleicht trotzdem richtig, nur die Umsetzung noch nicht ideal.
            \item Ebenso möglich wäre, anstelle ein echtes musikalisches Spektrum zu erstellen, stattdessen den aktuell erstellten Vektor, wie in \autoref{fig:nomalization} dargestellt, durch lineare Interpolation um weitere Punkte zu ergänzen. Mit anderen Worten, die Punkte in \autoref{fig:nomalization} werden verbunden. Auf diese Weise hätte die \ac{STFT} wesentlich mehr Spielraum.
            \item Auch denkbar wäre, mehrere \acp{KF} in den Vektor einzubeziehen. Allerdings hätte das möglicherweise den Nebeneffekt, dass beim Matching nicht \ac{KF}-spezifisch gesucht wird. Denn \protfin\ hätte theoretisch die Option, funktionelle Ähnlichkeit in Bezug auf spezielle physikalische Eigenschaften zu identifizieren.
        \end{enumerate}
        
        Es gibt allerdings auch noch Wege, wie die aktuelle Implementierung weiterentwickelt werden kann. So ist zum Beispiel das Family-Matching ausbaufähig. Neben den oben genannten Änderungen, könnte ebenfalls die Bewertungsmethode der Treffer erweitert werden. Aktuell ist deren Score lediglich die Anzahl Hashes, die mit den Hashes der Familie übereinstimmen, wie auf \autopageref{fam.matching} erläutert. Im Gegensatz zum Single-Protein-Matching wird hier die Position der Hashes also nicht einbezogen, was die Wahrscheinlichkeit auf Übereinstimmung deutlich erhöht. Der Grund ist, dass nicht klar ist, ob die Hashes auch innerhalb der Familie dieselben Positionen haben. Das müsste experimentell ermittelt werden. Ansonsten wäre es auch ein möglicher Ansatz, alle Positionen P zu speichern und bei der Identifizierung eines Treffers einen Hash nur als Übereinstimmung zu bewerten, wenn seine Position Teil von P ist.

        Das Speichern aller Positionen wäre ebenso eine Option für das Single-Protein-Matching. Zwar würde das ermitteln des S1-Score, siehe \autoref{fig:scoring}, deutlich aufwändiger sein, aber möglicherweise wäre dieser Score dann gar nicht mehr notwendig. Wenn bei der Generierung der Hashes in \autoref{alg:hashing} nicht die Position, sondern die Anzahl der Positionen gespeichert werden würde, wäre die Summe der Werte der Hashes die Gesamtzahl an Hashes des Proteins. Werden die Hashes zweier Proteine verglichen, so bildet die Summe der absoluten Differenzen der geteilten Hashes die Übereinstimmung beider Proteine. Das Verhältnis beider wäre konzeptuell dem \ac{JSI} ähnlich, nur dass die beiden Mengen Duplikate enthalten dürfen. Das Problem beim \ac{JSI} ist, dass er positionsunspezifisch ist. Dieser alternative Score ist ihm zwar ähnlich, aber da hier wirklich alle Hashes einfließen, da die Kombination aller Hashes die Positionen in abstrakter Form beinhaltet, da die Hashes prinzipiell Kanten in einem fast vollständigen Graph sind, wie in \autoref{fig:hashing} dargestellt. Hat man eine Kante, folgt zwangsläufig daraus, dass von den Enden ebenfalls weitere Kanten ausgehen, und mit der Vorgabe, nach welchem System die Punkte verbunden werden, entsteht trotzdem der ursprüngliche Graph. Demzufolge wäre es auch möglich, mit diesem Score die \ac{TZ} wieder zu erweitern, um einen vollständigeren Graph aus der Constellation-Map zu erstellen, wobei die Performanz des Scorings davon unberührt bliebe.

        Ein Problem, das bei den kleinen \ac{FG} angegangen werden muss, ist die Hashbildung. Bei einer maximalen \ac{FG} von 50 gibt es 26 Frequenzen, die als Minimum oder Maximum ausgewählt werden können. Bei einer \ac{TZ} von 8 sind das ${(26 \cdot 2)}^{2} \cdot 8 = 21632$ mögliche Kombinationen, die ein Hash annehmen kann. Davon ausgehend, dass \protfin\ mal mit Millionen von Proteinen verwendet werden soll, ist das viel zu unspezifisch. Sollte es bei diesen kleinen Fenstern bleiben, muss der Informationsgehalt eines Hashes deutlich erhöht werden. Hierfür könnten zum Beispiel statt Paaren von Punkten der Constellation-Map größere n-Tupel kombiniert werden, wobei hier die Gefahr zu hoher Spezifität besteht. Das oben beschriebene Einbeziehen aller Hash-Positionen wäre vielleicht auch eine Option für das Problem oder die Erweiterung der Vektoren.

        Es gibt also noch viel Entwicklungsbedarf und -möglichkeiten für \protfin. Die Identifikation einzelner Proteine funktioniert sehr gut, das Erkennen von funktioneller Ähnlichkeit hingegen weniger, wobei die Speicher-Komplexität des Trainings auch deutlich verringert werden muss. Doch wird ein Weg gefunden, das zu erreichen, so bietet der Algorithmus eine ganz neue Alternative für die sequenzbasierte Verwandtschaftsanalyse.
    % subsection ausblick (end)
% section diskussion (end)
