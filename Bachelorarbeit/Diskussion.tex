\section{Diskussion} % (fold)
    \label{sec:diskussion}
    Die Entwicklung von \protfin\ hat das Ziel, eine mögliche Alternative zu Alignment-basierten Verwandtschaftsanalysen zwischen Proteinen zu erhalten. Der zugundeliegende Algorithmus ist dabei von SHAZAM inspiriert, welches Musik erkennt, indem es Tonaufnahmen nicht auf direkte Ähnlichkeit vergleicht, wie es bei einem Alignment der Fall wäre, sondern durch Vergleich der periodischen Signale innerhalb der Aufnahmen. Dies wurde zum Teil erfolgreich bei \protfin\ umgesetzt, denn die Erkennung von Proteinen funktioniert zwar, dafür ist aber die Performanz hinsichtlich Speicherbedarf der Datenbanken und daraus resultierenden hohen Laufzeiten nicht gut. Außerdem fehlt bisher eine robuste und zuverlässige Möglichkeit, verwandte Proteine zu identifizieren. Um diesen Mangeln entgegenzuwirken, wurden \NumWord{\theexperiment} verschiedene Experimente entwickelt, die zum Ziel haben, nur möglichst signifikante Signale in die Datenbank einfließen zu lassen.

    Verglichen mit den Ergebnissen der vorigen Version von \protfin\ in \MyRef{fig:prev_results}, wurde in allen angesetzten Experimenten eine Verbesserung in den Datenbankgrößen erzielt, abgesehen von \acf{FG} 10, welche an Eindeutigkeit der Ergebnisse verloren hat. Es ist hier aber wichtig zu bemerken, dass in der Vorversion das Single-Protein-Matching lediglich für nur einen \acf{KF} durchgeführt wurde. Daher ergibt der Verlust bei \ac{FG} 10 Sinn, da die so wenigen infrage kommenden Frequenzen trotzdem auf über 100 \acs{MB} aufgebläht wurden. Bezüglich der Schärfe lässt sich hier leider kein Vergleich vornehmen, da dieses Maß in der Vorversion noch nicht implementiert war. Ebenso stammt der Ansatz des Family-Matchings auch erst aus dieser Arbeit.

    Das Sampling scheint eine gute Idee gewesen zu sein, um die Frequenzselektion signifikanter zu gestalten, ohne dabei willkürlich Information zu entfernen. In \MyRef{fig:frequencies_uniref} ist der Effekt auf die Wahl deutlich zu erkennen. Die Reduktion der Häufigkeit, mit der eine Frequenz gewählt wird, im Vergleich zu den vorigen Ergebnissen bei nur einem betrachteten \ac{KF} ist hier sehr stark erfolgt. So liegt der Maximalwert bei $\alpha=5\%$ und allen \acp{KF} bei etwa 45000 (\MyRef{fig:frequencies_5}) und in der Vorversion mit nur einem \ac{KF} bei fast dem Zehnfachen (\MyRef{fig:frequencies_normal}). Dennoch ähneln sich beide Varianten in der Hinsicht, dass Frequenz 0.1 am häufigsten auftaucht, also einer Periode von jeder 10. Aminosäure (Reziproke der Frequenz ist Periodenweite).

    Die geringere Anzahl Frequenzen zeigt sich in \MyRef{fig:uniref90.sp} in deutlich kleineren Datenbankgrößen als vorher (\MyRef{fig:prev_results}).

    Bezüglich der Werte für $\alpha \le 0.1\%$ ist es zweifelhaft, ob bei diesen niedrigen Häufigkeiten unter 200 wirklich alle \acf{TP} abgedeckt werden können, zumindest mit dem in \MyRef{exp:uniref90} verwendeten Selektionsansatz. Für $\alpha=5\%$ ist das Single-Protein-Matching in \MyRef{fig:uniref90.sp} jedenfalls noch vielversprechend.

    Da die Datenbankgrößen dennoch recht hoch sind, wäre hier nur eine Reduktion über andere Ansätze denkbar, wie zum Beispiel das Filtern von Hashes in \MyRef{exp:filter_hashes}. Die Ergebnisse dort zeigen, dass auf diese Weise effektiv kleinere Datenbanken erzielt werden, wobei die Schärfe und Identifikationsrate verlustfrei bleiben. Da bei dem Experiment zudem die gelernten Grenzwerte das alleinige Selektionskriterium darstellen, im Gegensatz zur Kombination mit Wahl lokaler Maxima wie bei \MyRef{exp:uniref90}, sollte dies für die Ergebnisse in \MyRef{fig:uniref90.sp} zu einer Verbesserung führen.

    Das Single-Protein-Matching bei \MyRef{exp:filter_hashes} hat gute Ergebnisse hervorgebracht. In \MyRef{fig:filter_hashes.sp} ist klar zu erkennen, wie das Filterquantil lediglich die Datenbankgröße beeinflusst. Es hat sich somit durch die kleinen Datenbanken bei kleinem Quantil gezeigt, dass die Hashes, die in sehr vielen Proteinen vorkommen, wenig Relevanz für das Single-Protein-Matching haben.

    Auch beim Family-Matching stechen die kleinen Quantile hervor. Das in \MyRef{fig:filter_hashes.fam} kleinste Quantil der Hashes von $10\%$ hat hier den besten Score. Allerdings ist der steigende F-Score mit kleiner werdendem Quantil dadurch zu erklären, dass die Wahrscheinlichkeit auf falsche Treffer sinkt, wenn weniger Hashes behalten werden. Die Mitglieder der Familie sind schließlich garantiert enthalten, sofern es Hashes gibt, die sie sich teilen.

    In \MyRef{exp:target_zone} wird dieselbe Selektionsmethode wie in \MyRef{exp:uniref90} verwendet. Es zeigt sich, dass eine \acf{TZ} von 8, also einem Bedarf von 3 Bit, vollkommen ausreichend für die Identifikation ist. Wie auch in den Ergebnissen zu \MyRef{exp:filter_hashes} in \MyRef{sub:filter_results} ist hier beim Single-Protein-Matching kein Verlust an Präzision bei der Erkennung von Proteinen sichtbar (siehe \MyRef{fig:target_zone.sp}). Insofern scheint die \ac{TZ} von 8 die beste Wahl zu sein, um eine kleine Datenbank zu erzielen. Lediglich \ac{FG} 10 hat eine schlechte Performanz, doch das hängt mit dem verwendeten Signifikanzniveau von 5\% zusammen, welches schon in dem repräsentativen Single-Protein-Matching in \MyRef{sub:uniref90} von \MyRef{exp:uniref90} eine schlechte Identifikation lieferte.

    Die insgesamt besten Ergebnisse hat \MyRef{exp:selection_method} erzielt. Von den drei definierten Ansätzen zur Frequenzselektion, gibt es in den ersten beiden davon Parameter, die zu einer Datenbankgröße führen, die der Eingabe entspricht, wobei die Identifikationsrate beim Single-Protein-Matching maximal ist (siehe \MyRef{fig:selection_method.none.sp} für Ansatz 1 und \MyRef{fig:selection_method.absolute.sp} für Ansatz 2) und auch das Family-Matching bessere Scores in diesen Parametern hat (siehe \MyRef{fig:selection_method.none.fam} für Ansatz 1 und \MyRef{fig:selection_method.absolute.fam} für Ansatz 2). Es handelt sich hierbei um das 0.001\%-Quantil für die Werte $k \in \{2, 3\}$ für alle getesteten \acp{FG} 30, 40 und 50.

    Die zusätzliche Vorselektion über die Extrema der Amplituden in Ansatz 2 (\MyRef{exp:selection_method}) scheint zudem einen glättenden Effekt auf die Verteilung der Datenbankgrößen zu haben, die Whisker der Boxen sind sehr klein.

    Der dritte Ansatz der Selektion ist nicht so vielversprechend. Für alle Parameter mit einem $\alpha\le 0.1\%$ sind die Datenbanken etwa halb so groß wie die Eingabe, haben beim Single-Protein-Matching eine geringe Schärfe und Identifikationsrate (siehe \MyRef{fig:selection_method.deviation.sp}) und beim Family-Matching eine Schärfe nahe null.

    Da hier offensichtlich eine sehr strenge Auswahl in den Frequenzen stattfindet, die die Datenbanken so drastisch verkleinert, wurden allerdings auch Ergebnisse für $\alpha=5\%$ erzielt, wofür bei den anderen Ansätzen die Datenbanken zu groß wurden. Beim Single-Protein-Matching wurde hier jedes Protein eindeutig identifiziert, und auch die Schärfe ist bei nahe 100\%. Beim Family-Matching unterscheiden sich die F1-Scores zwar nicht erheblich von den anderen Signifikanzniveaus, dafür ist aber auch hier die Schärfe etwas besser.

    Insgesamt waren alle \NumWord{\theexperiment} Experimente ein voller Erfolg:
    \begin{enumerate}
        \item \textbf{\Exp{exp:uniref90}:}\ \ Das Sampling hat seinen Zweck voll erfüllt. Die Frequenzen wurden seltener ausgewählt, blieben dabei aber weiterhin signifikant. Die war die Voraussetzung für die kleineren Datenbanken in den Folgeexperimenten.

        \item \textbf{\Exp{exp:filter_hashes}:}\ \ Das Filtern der Hashes hat wie \Exp{exp:uniref90} eine zuverlässige Reduktion der Datenbank bewirkt, ohne die Performanz zu beeinträchtigen.

        \item \textbf{\Exp{exp:target_zone}:}\ \ Es wurde gezeigt, dass selbst eine kleine \ac{TZ} von 8 für beim Single-Protein-Matching die Identifikation nicht beeinträchtigt und dabei die Datenbankgröße weiter verkleinert. Zudem wurde \ac{TZ} 8 auch in \Exp{exp:selection_method} verwendet (siehe \MyRef{tab:parameter}), und da wurden ebenfalls sehr gute Ergebnisse erzielt.

        \item \textbf{\Exp{exp:selection_method}:}\ \ In \Exp{exp:selection_method} haben sich die ersten beiden Ansätze als sehr vielversprechend erwiesen. Da sich beide in ihrem Konzept sehr ähneln, ist Ansatz 2 vielleicht für weitere Experimente geeigneter, da hier die Datenbankgrößen weniger streuten.

        Aber auch Ansatz 3 kann noch weiter getestet werden. In Kombination mit dem Filtern von Hashes wird vielleicht auch hier die Datenbankgröße entscheidend reduziert.
    \end{enumerate}

    Was in den Ergebnissen allerdings fehlt, ist beim Single-Protein-Matching der Bezug zur Familienähnlichkeit. Aktuell wird lediglich betrachtet, ob das Protein selbst identifiziert wurde und wie weit sich der Score von den Treffern abhebt, die nicht in der Familie sind. Letzteres wird durch die Schärfe abgebildet, die den Abstand prozentual angibt, also wie viel höher der Score ist. Dieser Abstand soll möglichst hoch sein, der zu den Familienmitgliedern hingegen nicht. Die Schärfe müsste um diese Information erweitert werden, was sich folgendermaßen formulieren lässt:
    \begin{equation}
        \label{equ:}
        \begin{split}
            scores\_trp &= \{\emph{S1}(t) \cdot JSI(t) \mid t \in TrP\}\\
            scores\_fp &= \{\emph{S1}(f) \cdot JSI(f) \mid f \in FP\}\\
            dist\_nicht\_familie &= \frac{\texttt{max}(scores\_trp) - \texttt{max}(scores\_fp)}{\texttt{max}(scores\_trp)}\\
            dist\_familie &= \frac{\texttt{max}(scores\_trp) - \texttt{mean}(scores\_trp \setminus \{\texttt{max}(scores\_trp)\})}{\texttt{max}(scores\_trp)}\\
            \emph{Schärfe} &= dist\_nicht\_familie \cdot (1 - dist\_familie)
        \end{split}
    \end{equation}

    $TrP$ sind die Treffer innerhalb der Familie und $FP$ die anderen. Der Abstand ($dist$) zu den $TrP$ ist, wie sehr sich der beste Score der Familie prozentual von allen anderen Familienmitgliedern abhebt. Damit dieser Abstand in die Schärfe minimierend einfließt, wird diese wie bisher berechnet und anschließend mit der Umkehrung des Familienabstands multipliziert, also wie nah der beste Score den $TrP$ ist. Auf diese Weise wäre ein hoher Abstand zu $FP$ bei ebenso hohem Abstand zu $TrP$ trotzdem schlecht bewertet. Gleiches gilt umgekehrt, dass eine hohe Nähe zu den Familienmitgliedern auch schlecht bewertet wird, wenn die $FP$ ebenso nah sind. Gegebenenfalls sollte der Abstand zu den $FP$ auch über den Mittelwert berechnet werden anstelle des Maximums, damit die Schärfe robuster gegenüber Ausreißern der $FP$ ist.

    Aktuell ist in allen Ergebnissen des Single-Protein-Matchings die Schärfe annähernd maximal, wenn alle Suchproteine eindeutig identifiziert wurden  (Unique Self Matches). Mit der beschriebenen erweiterten Schärfe wäre dies möglicherweise nicht mehr der Fall, sodass die einzelnen Methoden auch hinsichtlich Einfluss auf die Erkennung funktionaler Ähnlichkeit ausgewertet werden könnten.

    \subsection*{Ausblick} % (fold)
        \label{sub:ausblick}
        SHAZAM ist zweifellos sehr erfolgreich in der Erkennung von Musik mittels Tonaufnahmen, die aufgrund schlechter Mikrofonqualität sehr verzerrt sind. Durch das Aufteilen in mehrere Fenster, die einen Umfang von unter einer Sekunde haben, wird dies ermöglicht, da somit nicht relevant ist, wo die Aufnahme startet.

        Wie in der Einleitung (\autopageref{sec:einleitung}) erwähnt, sind Vektoren der digitalen Musik deutlich länger als Proteinsequenzen. Ein Fenster von einer halben Sekunde hätte etwa 20000 Werte. Bei einem angenommenen Originalsong von drei Minuten Länge (180 Sekunden) und 50\% Überlappung der Fenster wären das $\frac{180 \cdot (20000 \cdot 2)}{0.5 \cdot 20000}=720$ Fenster.

        Die Umsetzung dieses Verfahrens für Proteine führt aktuell in \protfin\ allerdings zu Fenstern von maximal Länge 50 (siehe \MyRef{tab:parameter}), was bei einer medialen Aminosäuresequenzlänge von 300 (\MyRef{fig:sequences}) bei auch 50\% Überlappung zu lediglich $\frac{300}{0.5 \cdot 50}=12$ Fenstern führt.

        Die kurzen Fenster führen zu einer geringen Anzahl überhaupt möglicher Frequenzen und die wenigen Fenster zu eingeschränkter Hashbildung. Es ist nicht auszuschließen, dass dieser aktuelle Ansatz der Übersetzung einer Aminosäuresequenz in einen numerischen Vektor nicht zielführend ist.

        Sollten weitere Experimente weiterhin eine schlechte Performanz hinsichtlich der Erkennung funktional Verwandter aufweisen, gibt es noch alternative Ansätze, den numerischen Vektor zu generieren:
        \begin{enumerate}
            \item Ein noch nicht weiterverfolgter Weg ist, die Sequenzen in echte Musik zu übersetzen, sodass SHAZAM direkt darauf angewandt werden könnte. \Anhang{A0A1D8EJF9.wav} ist eine Beispielmusikdatei, die in einem anfänglichen Test aus einer Aminosäuresequenz generiert wurde, wobei jede Aminosäure einem Akkord aus \acp{KF} entsprach.

            Ein solcher Akkord besteht folglich aus 10 Tönen, dessen Lautstärke durch den jeweiligen Wert eines \ac{KF} beeinflusst wird (näheres dazu siehe \Anhang{README.txt} im Abschnitt \texttt{Reproduzierbarkeit} zu \texttt{A0A1D8EJF9.wav}). Problem hierbei war ebenso die Datenbankgröße, da Musik komplexer als Text ist. Dennoch ist der Kern der Idee vielleicht trotzdem richtig, da die Aminosäuresequenz so den Umfang von tausenden Werten hätte, wie auch Musik, und SHAZAM direkt auf diese synthetische Aufnahme angewandt werden könnte. Die exakte Methode, wie die Übersetzung am besten erfolgt, müsste dann ausgetestet werden.

            \item Ebenso möglich wäre, anstelle synthetische Musik zu generieren, stattdessen den aktuell erstellten Vektor, wie in \MyRef{fig:normalization} dargestellt, durch lineare Interpolation um weitere Werte zu ergänzen. Auf diese Weise wären ebenfalls mehr Fenster möglich, abhängig davon, um wie viele Werte der Vektor erweitert wird.
        \end{enumerate}
        
        Es gibt allerdings auch noch Wege, wie die aktuelle Implementierung weiterentwickelt werden kann. So ist zum Beispiel das Family-Matching ausbaufähig. Neben den oben genannten Änderungen, könnte ebenfalls die Bewertungsmethode der Treffer erweitert werden. Aktuell ist deren Score lediglich die Anzahl Hashes, die mit den Hashes der Familie übereinstimmen, wie auf \autopageref{fam.matching} erläutert. Im Gegensatz zum Single-Protein-Matching wird hier die Position der Hashes also nicht einbezogen, was die Wahrscheinlichkeit auf Übereinstimmung deutlich erhöht. Der Grund ist, dass nicht klar ist, ob die Hashes auch innerhalb der Familie dieselben Positionen haben. Das müsste experimentell ermittelt werden. Andernfalls wäre es auch ein möglicher Ansatz, alle Positionen P zu speichern und bei der Identifizierung eines Treffers einen Hash nur als Übereinstimmung zu bewerten, wenn seine Position Teil von P ist.

        Das Speichern aller Positionen der Hashes wäre ebenso eine Option für das Single-Protein-Matching. Zwar würde das Ermitteln des S1-Score, siehe \MyRef{fig:scoring}, deutlich aufwändiger sein, aber möglicherweise wäre dieser Score dann gar nicht mehr notwendig. Wenn bei der Generierung der Hashes in \MyRef{alg:hashing} nicht die Position selbst, sondern die Anzahl der Positionen gespeichert werden würde, wäre die Summe der Werte der Hashes die Gesamtzahl an Hashes des Proteins. Werden die Hashes zweier Proteine verglichen, so bildet die Summe der absoluten Differenzen der geteilten Hashes die Übereinstimmung beider Proteine.

        Das Verhältnis beider wäre konzeptuell dem \ac{JSI} ähnlich, nur dass die beiden Mengen Duplikate enthalten dürfen. Das Problem, dass der \ac{JSI} positionsunspezifisch ist, wird hier dadurch ausgeglichen, dass wirklich alle Hashes in den Score einfließen. Denn die Kombination aller Hashes beinhaltet deren Positionen in abstrakter Form, da die Hashes prinzipiell Kanten in einem fast vollständigen Graph sind, wie in \MyRef{fig:hashing} dargestellt. Hat man eine Kante, folgt zwangsläufig daraus, dass von den Enden ebenfalls weitere Kanten ausgehen, und mit der Vorgabe, nach welchem System die Punkte verbunden werden, entsteht trotzdem der ursprüngliche Graph. Demzufolge wäre es auch möglich, mit diesem Score die \ac{TZ} wieder zu erweitern, um einen vollständigeren Graph aus der \acf{CM} zu erstellen, wobei die Performanz des Scorings davon unberührt bliebe.

        Ein Problem, das bei den kleinen \acp{FG} angegangen werden muss, ist die Hashbildung. Bei einer maximalen \ac{FG} von 50 gibt es 26 Frequenzen, die als Minimum oder Maximum ausgewählt werden können. Bei einer \ac{TZ} von 8 sind das ${(26 \cdot 2)}^{2} \cdot 8 = 21632$ mögliche Kombinationen, die ein Hash annehmen kann. Davon ausgehend, dass \protfin\ mit öffentlichen Datenbanken wie der UniProtKB \autocite{uniprot} verwendet werden soll, welche in Version \texttt{2024\_04} vom 24.07.2024 über 200 Millionen von Einträgen in TrEMBL hat und über 500 Tausend in Swiss-Prot, ist das viel zu unspezifisch.

        Sollte es bei diesen kleinen Fenstern bleiben, muss der Informationsgehalt \autocite{shannon} eines Hashes deutlich erhöht werden. Hierfür könnten zum Beispiel statt Paaren von Punkten der \ac{CM} größere n-Tupel kombiniert werden, wobei hier die Gefahr zu hoher Spezifität besteht. Dennoch würde die Anzahl möglicher Kombinationen in Abhängigkeit von $n$ exponentiell steigen ($(26 \cdot 2)^{n}\cdot 8$) und entspräche zum Beispiel für Quadrupel schon 7311616, was deutlich mehr ist. Das oben beschriebene Einbeziehen aller Hash-Positionen wäre vielleicht auch eine Option für das Problem oder die Erweiterung der Vektoren, weil Ersteres die Häufigkeit eines Hashes einbringt, welche beim Scoring relevant ist, und Letzteres die Anzahl Fenster oder maximal möglicher Frequenzen erhöht.

        Diese Betrachtungen zeigen, dass noch einige Experimente möglich sind, bevor das Konzept, die von SHAZAM inspirierte Spektralanalyse auf  Proteinsequenzen anzuwenden, als ungeeignet abzulehnen wäre. Die Identifikation einzelner Proteine funktioniert sehr gut, was allerdings bisher nur mit vollständigen Suchsequenzen getestet wurde. Das Erkennen von funktioneller Ähnlichkeit ist hingegen noch schwach. Doch wird ein Weg gefunden, besonders Letzteres zu verbessern, so bietet der Algorithmus eine ganz neue Alternative für die sequenzbasierte Verwandtschaftsanalyse.
    % subsection ausblick (end)
% section diskussion (end)
